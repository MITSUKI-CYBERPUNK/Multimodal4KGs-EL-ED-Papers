# 0 摘要
我们首先定义了 KG 和 MMKG，然后探讨了它们的构建进展。我们的综述包括两个主要任务类别： 
+ KG感知的多模态学习任务（如图像分类和视觉问题解答）
+ MMKG固有任务（如多模态知识图谱补全和实体对齐）
对于其中的大多数任务，我们提供了定义和评估基准，此外还概述了开展相关研究的基本见解。最后，我们讨论了当前面临的挑战并确定了新兴趋势，例如大型语言建模和多模态预培训策略方面的进展。

# 1 引入
 + **知识和多模态本质上是相辅相成的**
# 2 预备
## KGs:
+ **定义 1：知识图谱**。
	+ 知识图谱（KG）表示为 **G = {E, R, T }**，由实体集 E、关系集 R 和语句集 T 组成。
	+ 语句既可以是关系事实三元组（h、r、t），也可以是属性三元组（e、a、v）。
	+ 具体来说，KG 由一组关系事实组成一个多关系图，其中节点表示实体（E 中的 h 和 t 分别表示头实体和尾实体），边表示关系（r∈ R）。关于属性三元组，属性 a（a∈ A）表示实体 e 具有某个属性，该属性具有相应的值 v（v∈ V）。这些值可以包括各种文字，如字符串或日期，也可以包括标签和文本定义等元数据，通过内置或自定义注释属性来表示。
+ 本体论：RDF, RDFS, and OWL语言
+ KG范围拓展：
	+ WordNet：定义词与词之间相互关系的词汇数据库
	+ ConceptNet：归档了由不同术语相互关联的常识性知识
	+ 本体本身
	+ 任何以图格式组织的、节点具有明确语义解释的结构化数据(如语义网络)

## MMlearning:
+ 模态 "通常是指**特定类型的数据或信息渠道，以感官输入或表示格式为特征**"
+ 多模态学习通常旨在**开发一种从多种模态到输出空间的统一表征或映射，利用不同模态之间的互补性和冗余性来改进预测**。所面临的挑战在于**如何有效地调整、融合和整合来自不同模态的信息**，从而凝聚力量，发挥集体优势。
+ 与多视角学习的区别： 
	+ 多视角分析表明，每个视角（例如一朵花的不同视角）都能独立得出准确的预测结果，而多模态学习则不同，多模态学习所面临的场景是，缺少一种模态可能会妨碍任务的完成（例如缺少图像的视觉问题解答场景）。
	+ 此外，多视角学习通常涉及同一数据类型的不同视角，源自单一来源，如图像数据的不同特征。相比之下，多模态学习处理的是不同的数据类型，如源自**多个来源**的文本和图像。
+ **定义 2：多模态学习**。![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223134501.png)
	+ 假设给定数据 xˆ = x(1), ... . , x(K) 由 K 种模态组成，x(k) ∈ X (k) 表示第 k 种模态的域集，X = X (1) × - - × X (K)。让 Y 表示目标域，Z 表示潜空间。表示 g : X 7→ Z 为从输入空间（利用所有 K 个模态）到潜在空间的真实映射，q ： Z 7→ Y 为真正的任务映射。例如，在基于聚合的多模态融合中，g 是建立在 K 个独立子网络基础上的聚合函数，q 是一个多层神经网络。在学习任务中，数据对（xˆ，y）∈ X × Y 是由未知分布 D 生成的
	+ 其中 q o g(xˆ) = q(g(xˆ)) 表示 q 和 g 的复合函数。
+ 范围：由于重点关注两种模式（即语言和视觉），输入域简化为 X = X l × X v 和 xˆ = (xl, xv)，其中 xl∈ X l 和 xv∈ X v 分别表示来自语言和视觉域的输入数据![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223134725.png)

## KG4MM设置:
1) 子知识库提取： 实际应用往往需要利用本地化知识来有效解决特定任务。这通常需要检索、路由或语义解析算法。
2) 面向任务的 KG 构建： 从头开始构建特定任务的 KG，而不是使用现有的 KG 或子 KG 提取。
	(i) 静态领域 KGs 构建： 创建稳定的、特定领域的 KG，其中包含预定义的实体和关系，封装了重要的背景知识。
	(ii) 动态临时 KGs 构建： 在任务执行过程中构建动态临时 KG，利用 KG 推理算法为任务提供支持。

## MM4KG设置：
+ **定义 3：多模态知识图谱**。
+ 与定义 1 一致，KG 定义为 G = {E, R, A, T , V}，其中 T = {TA, TR}，TR = E × R × E，TA = E × A × V。 
+ (i) A-MMKG 利用多模态数据（如图像）作为实体或概念的具体属性值，TA = E × A × (VKG ∪ VMM )，其中 VKG 和 VMM 分别是 KG 和多模态数据的值。
+ (ii) NMMKG 将多模态数据视为 KG 实体，TR=（EKG ∪ EMM ）×R×（EKG ∪ EMM ），将典型 KG 实体（EKG）与多模态实体（EMM）分开。![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223140145.png)![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223140231.png)
请注意，N-MMKG 和 A-MMKG 并不是严格排斥的： N-MMKG 可被视为 A-MMKG 的一种特殊情况，尤其是当 A-MMKG 中的实体采用图像的形式，从而转化为 N-MMKG 时。鉴于数据访问的便利性以及与传统 KG 的相似性，A-MMKG 构成了当前 MMKG 研究中大多数应用和方法的基础

# 3 KG结构
## 典型KG结构
+ 基于实体的知识图谱Entity-based KGs：
	+ 在构建基于实体的知识图谱时，本体和数据都要遵守严格的标准，其中知识图谱节点通常代表与现实世界对象一一对应的实体。
	+ 这些知识库通常建立在人工定义的本体之上，语义清晰，实体类型和关系中的歧义和重叠极少。因此，每个领域的实体和关系数量相对较少，便于人工定义。
	+ 整合来自不同结构化来源的知识需要处理三种异构类型：
		+ (i) 模式异构，不同数据源可能以不同方式表示相同的实体类型和关系；
		+ (ii) 实体异构，不同来源的名称可能描述相同的现实世界实体；
		+ (iii) 值异构，不同来源可能为相同实体提供不同或过时的属性值。
		+ 为解决这些问题，相关研究任务层出不穷，包括**不完整知识图谱中的实体链接**和**跨不同知识图谱的数据融合（如知识图谱补全和实体对齐）**。此外，扩展知识图谱内容的技术还包括从网站等半结构化数据中**提取知识**。在这种情况下，每个页面通常代表一个主题实体，信息以键值对的形式显示，并在不同页面之间保持一致。这些技术旨在获取长尾知识，通常使用**人工构建的提取模式和监督提取算法**。
+ 富文本 text-rich KGs：
	+ 与基于实体的 KGs 不同，富文本 KGs **具有主要的文本属性**，在提取干净、无歧义的实体方面面临挑战，这使它们更类似于**双向图**而不是传统的连通图。
	+ 通常情况下，它们**能容忍更大的模糊性**，将节点表示为自由文本，而不是定义明确的实体，这使它们特别适用于产品和百科全书等领域，因为在这些领域中，值和类之间的语义区分往往不明确。
	+ 一般都依赖于提取模型。这些模型从相关的非结构化源数据中提取结构信息，采用**命名实体识别**方法来识别表明特定属性的模式。

## MMKG结构
+ MMKG 构建范例： MMKG 结构的两个主要范式：
	+ (i) 用知识图谱中的符号标注图像；
		+ 优先提取视觉实体/概念、关系和事件
		+ 在表示不常见的（即长尾）多模态知识时遇到了挑战，这主要是由于现实世界中常见的实体在不同语境中反复出现。监督方法的使用进一步加剧了这些挑战，因为它们本身就受到预先存在标签的有限范围的限制。
		+ 此外，该系统需要大量的预处理，包括制定特定规则、创建预定的实体列表以及应用预先训练好的检测器和分类器
	+ (ii) 将知识图谱符号与图像接地。 
		+ 目前大多数 MMKG 的典型构建范式是将知识图谱符号与图像接地，其中包括：
			+ 实体接地（即从在线资源中将实体与相应图像关联起来）
			+ 概念接地（即为视觉概念选择多样化、有代表性的图像，并抽象出共同的视觉特征）
			+ 关系接地（即选择在语义上反映输入三元组关系的图像）
+ MMKG发展：
	+ 值得注意的是，最早的一般意义上的 MMKG 可以追溯到 ImageNet（2009 年）[74]，这是一个基于 WordNet [15] 结构的大规模图像本体。尽管 ImageNet 具有丰富的语义层次和数百万张注释图像，但作为一个 A-MMKG，它主要用于对象分类，其知识组件往往未得到充分利用。
	+ NEIL（2013 年）[42] 是通过关系提取、数据标注和分类器/检测器学习循环从互联网构建视觉知识的早期尝试。然而，NEIL 的可扩展性有限，这体现在它需要大量计算来对 2273 个对象的 40 万个视觉实例进行分类，而典型的知识库需要数十亿个实例的基础。
	+ 进一步的发展[70]、[75]-[78]侧重于改进复杂图像的视觉检测和物体分割，Chen 等人[75]利用从视觉子类别中学习到的自上而下的分割先验来帮助构建。
	+ Visual Genome（2016）[49] 提供了对象、属性和关系的密集注释。不过，它主要是辅助图像描述和问题解答等场景理解任务。
	+ ImageGraph（2017）[43]植根于Freebase[13]，而IMGpedia（2017）[53]则将维基共享资源6的视觉数据与DBpedia元数据联系起来，是对MMKGs的进一步扩展。ImageGraph是通过网络爬虫解析图像搜索结果并应用启发式数据清理规则（如重复数据删除和排序）组装而成的，其重点是对视觉概念进行推理，从而实现关系预测和多关系图像检索。作为一个 N-MMKG，IMGpedia 强调视觉描述符和相似性关系，支持视觉语义查询，但受到常识和百科知识范围的限制。
	+ 2019年，Liu等人[54]首次正式提出了 “MMKG ”一词，推出了三个用于链接预测和实体匹配研究的A-MMKG数据集，这些数据集以Freebase15K（FB15K）[79]为基础，使用网络爬虫作为图像收集器构建而成，平均每个实体有55.8幅图像。
	+ 同时，DBpedia15k（DBP15K）和 Yago15k（YG15K）是通过将 DBpedia 和 Yago 中的实体与 FB15K 对齐，用数字字面、图像信息和用于跨 KG 实体链接的 sameAs 谓词来丰富这些知识图谱而开发的。
	+ GAIA（2020）[41] 是一个 MMKG 提取系统，支持复杂图查询和多媒体信息检索。它在相同的文档集上集成了文本知识提取和视觉知识提取流程，生成特定模态的 KG，然后将其合并为一个连贯的 MMKG。与此同时，VisualSem [44] 作为 A-MMKG 出现，它从 BabelNet 中获取实体和图像[80]进行细致过滤，以确保数据质量和多样性。
	+ VisualSem 中的实体与维基百科、WordNet 词组[15]以及 ImageNet 高分辨率图像[74]相连接。作为 N-MMKG，Richpedia[58] 从维基百科[81] 收集图像和描述，使用超链接和文本手动识别图像实体之间的关系，并辅以网络爬虫收集更广泛的图像实体。
	+ 最近，MMKG 社区的重点已从构建转向应用，强调 MMKG 表征学习（§ V-A）、获取（§ V-B）、融合（§ V-C）、推理（§ V-D）和 MMKG 驱动的应用（§ V-E）等领域。
	+ 虽然 MMKG 采集扩展了构建工作，但它主要解决的是多模态提取挑战[40]，这凸显了大规模 MMKG 资源的稀缺性，以及对特定任务数据集的需求，以解决 MMKG 的局限性并支持新型下游任务。 
+ N-MMKG 本体论：考虑到 A-MMKG 本体大体上反映了标准 KG，主要区别在于包含了视觉属性，我们在这一部分主要讨论几个有代表性的 N-MMKG 本体。
+ 公开KGs资源：![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223165742.png)


# 4 MM4KG 任务
MMKG 的构建过程反映了人类的认知操作，包括信息的获取、融合和推理。在整个发展过程中，各种任务（即 In-MMKG 任务）被确定下来，从而将 MMKG 定位为处理一系列下游多模态任务的基石。![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223163130.png)
![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223165534.png)

## 4.1 MMKG Acquistion 获取
MMKG 获取（或提取）是指通过整合文本、图像、音频和视频等多模态数据来创建 MMKG。这一过程利用互联网搜索引擎或公共数据库等其他来源的多模态信息来增强现有的知识图谱或开发新的 MMKG，从而实现对复杂、相互关联的概念的全面理解。由此产生的 MMKG 可利用每种模式的独特优势，提供更具凝聚力和更详细的知识表征。
### 4.1.1 MNER & MMRE 多模态实体识别和关系抽取
![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223163702.png)
+ 多模态命名实体识别（MNER）通过结合视觉信息对其进行了扩展，大大增强了多模态语境下的 NER。
+ **定义 8：多模态命名实体识别**：多模态命名实体识别（MNER）通常被视为一个序列标注问题，其中一个模型将句子 xl = {w1, w2, . , wL} 以及相关图像 xv 作为输入，以确定文本中是否存在命名实体以及命名实体的类型。MNER 的目标是预测标签序列 Y = {y1, . , yn}，其中每个标签 yi 对应句子中每个标记 wi 的命名实体类别。这一过程，包括标签序列的概率计算，都遵循 NER 中的基本序列标注技术![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223164035.png)
+ MNER 方法的演变： 整合视觉和文本信息的方法多种多样，这标志着 MNER 的进步。
	+ (i) 基于 BiLSTM 的方法： 早期的研究 主要采用**模态注意力网络**来融合文本和图像特征，在 LSTM 中引入视觉注意力门，以增强对社交媒体帖子中命名实体的理解。
	+ (ii) 基于 PLM(Tranformer系) 的方法： 
		+ 其中，**基于编码器的 PLM（如 BERT）** 率先应用于 MNER，其重点是设计模态融合方法，以提高纯文本 NER 性能，同时最大限度地减少视觉噪声。
		+ **UMT**通过添加一个用于额外上下文文本表示的 Transformer 层和一个用于视觉整合的跨模态 Transformer 层，将标准 BERT 架构调整为 MNER。研究表明，**视觉表征有助于识别实体类型，但不利于检测实体跨度**。因此，UMT 包含一个**基于文本的辅助模块**，专门用于更准确的实体跨度检测。
		+ **FMIT**利用**扁平网格结构**和**相对位置编码**，实现了不同模态的细粒度语义单元之间的直接交互。
		+ **MAF** [375] 包含一个跨模态匹配模块，用于计算文本与图像之间的相似度得分，并利用该得分调整视觉信息的整合量。
		+ 此外，跨模态对齐模块可对齐两种模态的表征，创建统一的表征，弥合语义鸿沟，促进文本与图像之间更好的连接。**ITA** [390] 将图像 Transformer 为文本对象标签和标题，用于跨模态输入，使纯文本 PLM 能够有效地模拟模态之间的交互，并提高对图像相关噪声的鲁棒性。
		+ Wang 等人[372]进一步提出了一种基于 Transformer 的瓶颈融合机制，该机制**只允许模态通过可训练的瓶颈标记进行交互，从而限制了噪声的传播**。
		+ **CATMNER** [392] 利用实体标签衍生的显著性得分来完善注意力机制，从而解决跨模态交流中的复杂问题。
		+ **MoRe** [485] 利用多模态检索框架，通过不同的文本和图像检索器分别收集相关段落和相关图像。这些数据分别用于训练 NER 和 RE 任务的独立模型，然后由一个**专家混合（MoE）模块**将它们的预测结果协同起来。
		+ **TISGF** [384] 创建了视觉和文本场景图，并对其进行编码，以提取跨模态的对象级和关系级特征。然后，它采用文本图像相似性模块来确定视觉信息的融合程度。最后，使用融合模块整合多模态特征，并由**条件随机场（CRF）** 确定实体类型。
		+ **PromptMNER** [391] 利用与实体相关的**提示**，通过 CLIP [486] VLM 评估其与图像的匹配程度，从而提取视觉线索。
		+ **MGICL** [394] 分析**不同粒度**的数据，包括文本的句子和单词标记级别，以及视觉图像和对象级别。其跨模态对比方法利用视觉特征增强了文本分析，并辅以**视觉门机制**过滤噪音。
	+ (iii) 特殊情况： 一些著作强调了 MNER 中的特殊情况。
		+ 例如，Liu 等人[487] 建议**在 MNER 中整合不确定性估计**，以提高预测可靠性。
		+ **DebiasCL** [386] 则侧重于通过视觉对象密度引导的硬样本挖掘策略和去势对比损失来减轻 MNER 中的偏差。
		+ 基于编码器解码器的 PLM，如 **T5** [197] 和 **BART** [295]，以其在 NLU 和 NLG 中的优势而闻名，在最近的 MNER 研究中也越来越受欢迎。
		+ Wang 等人[488] 引入了**细粒度 NER** 和**接地（FMNERG）任务**，该任务涉及提取文本中的命名实体、实体的详细类型以及图像中相应的视觉对象。在这里，（实体、类型、对象）三元组被转换成一个目标序列，T5 被用来生成这个序列，并结合线性变换层将视觉对象表征适应到 T5 的语义空间中。
+ ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223165802.png)



+ 关系提取（RE）是指对文本中实体之间的语义关系进行检测和分类。多模态关系提取（MMRE）整合了视觉信息，丰富了文本关系分析，在新闻文章分析等应用中证明是有效的，因为在这些应用中，文本与相关图片或视频相结合。
+ **定义 9：多模态关系提取**。MMRE 分析句子 xl = {w1, w2, ... , wL} 以及相应的图像 xv，重点关注句子中的一对实体（e1, e2）。任务包括利用文本和视觉线索（如图像中描绘的物体相互作用）对这些实体之间的关系进行分类。对于每个潜在关系 ri∈ R，都会分配一个置信度分数 p(ri|e1,e2,xl,xv)。关系集 R = {r1, . , rC , None} 包括预定义的关系类型，“None ”表示没有特定关系。![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223164136.png)
+ MMRE 方法的演变： 
	+ MMRE 评估文本内容中实体对之间的潜在关系，利用额外的多模态信息（如图像）捕捉补充信息，以实现更准确的关系分类。
	+ Zheng 等人[398] 首次证明了**多模态数据在填补语义空白和增强社交媒体文本分析方面的优势**。
	+ 基于这一概念，一些著作[367]、[400]引入了**文本-视觉关系对齐方法**，将句子解析树和视觉场景图对齐，从而改进了文本关系识别。
	+ 对于那些基于 PLM 的方法，**HVPNet**[377]引入了对象级视觉信息，采用分层视觉特征和视觉前缀引导融合来增强整合；
	+ **DGF-PT**[401]实现了一个双门融合模块，使用局部和全局视觉门来过滤无用的视觉数据，然后是一个生成解码器，利用实体类型来完善候选关系，从而为 MMRE 捕捉有意义的视觉线索。
+ ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223165815.png)

+ MNER 和 MRE 之间的重叠： 通常情况下，MNER 和 MMRE 都通过结合视觉信息来增强文本分析，但它们的侧重点不同： 
	+ MNER 侧重于识别实体
	+ MMRE 侧重于对这些实体之间的关系进行分类
	+ 在 MMKG 构建框架中，MMRE 可被视为 MNER 的后续任务。
	+ 尽管存在这些差异，但这些任务的开发方法正日益趋同，许多研究对 MNER 和 MMRE 都采用了类似的模型设计 [377]、[393]、[396]。


### 4.1.2 事件抽取
![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223165844.png)

## 4.2 MMKG Fusion 融合
互联网上异构数据的激增导致了无数独立 MMKG 的产生。整合这些来自不同数据源的 MMKG 至关重要，这使得 MMKG 融合成为 MMKG 构建的关键阶段 [504]。这一过程涉及多种任务，包括多模态实体对齐（MMEA）、实体链接（MMEL）和实体消歧（MMED）。
### 4.2.1 多模态实体对齐 (Multi-modal Entity Alignment)
![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223165934.png)
实体对齐（EA）是知识图谱整合的关键，其目的是**利用实体的关系、属性和字面（表面）特征来匹配不同知识图谱中的相同实体**。具体来说，
+ 符号逻辑方法[505]、[506]应用人工定义的规则，如逻辑推理和词汇匹配，来指导对齐。
+ 基于嵌入的方法[507]-[514]则利用学习到的实体嵌入来加快配准，从而避免了对预定义启发式方法的需求。
+ 多模式实体对齐（MMEA）结合了 MMKGs 中的视觉数据，将每个实体与图像关联起来，以增强 EA [54]。
**定义 11：多模态实体对齐**。MMKG 表示为 G = {E, R, A, T , V}，其中 T = {TA, TR}。给定两个对齐的 A-MMKG G1 = {E1, R1, A1, V1, T1} 和 G2 = {E2, R2, A2, V2, T2}，MMEA 的目标是分别从 E1 和 E2 中识别出代表同一真实世界实体 ei 的实体对 (e1 i , e2 i)。一组预先对齐的实体对作为参考，分为训练集（种子对齐 S）和测试集 Ste，按预定义的种子对齐比率 Rsa 进行配比。与实体相关的可用模式用 M = {g、r、a、v、s} 表示，分别代表图结构、关系、属性、视觉和表面（即实体名称）模式。![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223170310.png)
虽然**关系模态、属性模态和表面模态**都可以归入语言模态，但在 MMEA 社区中，它们经常被区分为不同的模态[55], [65], [415], [416], [418], [423], [425], [426] 。此外，研究显示了多种模态使用模式：
+ 一些研究只关注对齐过程中的**属性和关系类型**[65], [425], 
+ 而另一些研究则通过使用 **PLM**（如 BERT [175]) [420], [421], [515]-[518] 或**词嵌入**（如 Glove [163]) [55], [65], [414], [415], [425] 将其文本内容纳入实体表示。
+ 此外，有些方法是针对**只有一幅图像的实体**提出的[55]、[415]，
+ 而另一些方法则是**为了处理每个实体的图像数量可能是多幅[63]甚至是缺失[65]的情况**。
进展：目前的 MMEA 研究按其基本动机可大致分为两个流派：
+ （i）探索更好的跨知识图谱模态特征融合。
+   (ii) 分析 MMKG 对齐中的实际限制和挑战。
### 4.2.2 多模态实体链接与消歧 (Multi-modal Entity Linking & Disambiguation)![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223165952.png)
实体链接（Entity Linking，EL）是各种应用[523]-[525]中的重要组成部分，包括问题解答、关系提取和语义搜索。**实体链接的主要目标是将文档中的文本提及与知识图谱（如 Freebase [13]）中的相应实体关联起来。值得注意的是，提及的范围超出了文本形式，包括图像、音频和视频内容，所有这些都可以与知识图谱中的实体相关联**。多模态实体链接（MMEL）的最新研究发现，利用多模态信息可以显著提高传统 EL 方法的效率。
**定义 12：多模态实体链接**。MMKG 表示为 G = {E, R, A, T , V}，其中 E = {e1, e2, ..., ei} 为实体集。M = {g, r, a, v, s} 分别是图结构、关系、属性、视觉和表面信息。例如，xes1 , xev1 分别表示 e1 的名称和视觉信息。提及集定义为 N = {m1，...，mi}，其中 {xsm1 , ..., xsmi }, {xvm1 , ..., xvmi } 是相应的名称和视觉信息。**MMEL 的目标是根据多模态信息（xes1 , ..., xev1 , xsm1 , ..., xvm1 ）确定实体与提及（以 (ei, mi) 表示）之间的联系。**![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223171130.png)
进展： 
+ 早期的 MMEL 研究[428]、[430]、[526]侧重于**融合和扩展多模态数据**，例如合并来自媒体帖子的视觉和文本元素，以增强文本提及并预测相应的知识库实体。
	+ 例如，DZMNED[428] **利用知识图谱嵌入以及词级和字符级词性嵌入的混合**，这种策略巧妙地应对了在测试过程中识别以前未见实体的挑战。
	+ Zhang 等人[526]的研究重点是**去除噪声图像以提高性能**。
	+ 随后的研究对这些方法进行了扩展，探索了整合不同多模态上下文的策略，并开发了更合理的**多模态数据集** [432]、[434]、[435]、[527]-[530]。例如，**GHMFC** [432] 采用**门控融合和对比训练来改进提及表征**，而 **MIMIC** [435] 则引入了**多粒度交互网络来进行通用特征提取**。**AMELI** [530] 实现了**实体候选检索管道**，利用属性信息增强了 MMEL 模型。
+ 最近的 MMEL 探索主要采用 **(V)PLM** 进行特征表示。
	+ **BERT** [175] 常用于文本处理 [434]、[437]，而 **CLIP** [486] 则是视觉编码的首选 [438]、[439]。通常情况下，这些（V）PLM 的大部分参数保持**冻结**，辅以重点**微调**策略。
		+ 其中，**GEMEL** [438] 有效地结合了用于语言处理的 LLaMA [189] 和用于视觉编码的 CLIP，显示了 GPT 3.5 在 MMEL 中的潜力。
		+ Yang等人[434]介绍了一种**多提及MMEL**任务，该任务将**同一上下文中的不同提及视为单一样本，采用多提及协同排序法进行测试，以发现提及之间的潜在联系**。
		+ Pan 等人[436]提出了**多模态条目链接（Multi-modal Itemaspect Linking）**，重点是将短视频链接到短视频百科全书中的相关条目。
		+ **GDMM** [437] 采用**多模态编码器-解码器范式**，将文本、图像和表格三种模态结合在一起，从而接近 MMEL。
		+ **DWE** [439] 利用**详细的图像属性（如面部特征和场景特征）增强视觉特征**，利用维基百科的描述增强文本表述，从而弥合了文本和知识图谱实体之间的差距。

资源与基准： 
+ (i) **SnapCaptionsKB** [428]： 这是一个 MMEL 数据集，包含 12,000 个人工标注的图像字幕对，旨在捕捉各种多模式交互。由于《一般数据保护条例》（GDPR）的原因，**目前无法使用**。作为回应，Adjali 等人[430] 开发了一种从 Twitter 自动构建 MMEL 数据集的工具。
+ (ii) **M3EL** [527]： 该数据集包含 181 240 个与电影相关的文本提及和 45 297 张图片，提供**细粒度注释**。
+ (iii) **NYTimes-MEL** [434]： 源自《纽约时报》[288]、[531] 的图片和标题，侧重于 PERSON 实体。StanfordNLP 工具[532]用于标题中的 NER，其中一些实体被替换为昵称，以构建提及。与文献[432]类似，该工具也从维基数据[533]中丰富了每个实体的图像和 14 个属性，排除了无效实体或没有相应图像的样本。
+ (iv) **基于维基数据的数据集**： 包括 **WikiDiverse** [534] 和 **WikiMEL** [432]，这些数据集提供了跨越不同主题和实体类型的人工标注提及。WikiDiverse 包含体育和技术等维基新闻类别的数据，而 WikiMEL 整理了维基百科和维基数据中的提及信息。



**多模态实体消歧**： 在许多研究中，由于 EL 和实体消歧（ED）在方法和任务设置上的相似性，它们常常被视为同义词 [428]、[435]。然而，区分两者是至关重要的。**EL 包括将文本中的命名实体与知识图谱中的相应实体进行识别和链接的广泛过程，而 ED 则专门侧重于解决一个命名实体可能对应多个潜在候选实体的情况**。在 ED 中，每个数据集样本通常包括一个命名实体和一组相似度很高的候选实体，这就突出了任务的重点--**在这些选项中进行消歧**[428]。
+ EL 包括将文本中的命名实体与知识图谱中的相应条目进行识别和链接的广泛过程。相比之下，当一个命名实体可能与多个候选实体匹配时，ED 的具体目标是解决歧义问题。ED 强调在这些潜在候选实体中进行消歧，通常在每个数据集样本中都会出现一个命名实体和一组密切相关的选项。在多模态实体消歧（MMED）中，各种方法不仅利用文本信息，还利用视觉信息来完善消歧。
	+ 例如，**DZMNED** [428] 利用**卷积 LSTM** 来整合多模态数据。
	+ ET [430] 采用**树外分类器**来有效区分模棱两可的候选词。
	+ **IMN** [433] 采用**元学习**来获取多模态知识，并采用知识引导的迁移学习策略，有助于提取跨模态的内聚表征。
## 4.3 MMKG Inference 推理
MMKG 数据本身包含缺失元素、错误和矛盾，因此推理是完成知识图谱的关键任务。这一阶段是继 MMKG 构建周期中的提取和融合之后的又一阶段，旨在增强模型的推理能力，加深模型对知识图谱整体知识的理解。
### 4.3.1 多模态知识图谱补全 (Multi-modal Knowledge Graph Completion)
![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223170031.png)

### 4.3.2 多模态知识图谱推理 (Multi-modal Knowledge Graphs Reasoning) 