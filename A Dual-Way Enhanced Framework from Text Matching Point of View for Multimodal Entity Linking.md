# 1 Motivation
+ 模态不纯 ^8f3bcf
	+ 图像模态通常包含较少的信息和更多的冗余特征。高达33.8%的视觉信息可能与任务无关或甚至是噪声
	+ 现有的MEL工作可能受到视觉枢轴特征建模不力的影响
+ 实体表示模糊性：不同的人可能具有相同的属性，这使得即使使用理想的多模态方法，也很难链接实体。因此，需要**更独特和具有代表性的实体表示**来进一步消除歧义。
于是利用维基百科描述丰富原始数据语义，引入细粒度的多模态信息增强和实体语义的丰富来提高MEL任务的准确性和效率

# 2 Contribution
![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223214245.png)
1. **双路径增强框架(DWE)**：该框架从神经文本匹配的角度出发，将每个多模态信息（文本和图像）视为查询，并学习从每个查询到候选实体中相关实体的映射
2. **引入细粒度图像属性**：例如面部特征和场景特征（形容词-名词对，ANPS），这些属性被用来增强和提炼视觉特征，并采用跨模态对齐来弥合文本和视觉特征之间的语义差距。
3. **利用维基百科描述**：丰富实体的语义表示，减少文本表示和知识图中实体之间的差异，从而获得更全面的文本表示
4. **多模态信息增强查询**：设计了三个增强单元（文本增强、属性增强和视觉增强单元），基于**跨模态增强器**构建，以捕获多模态信息之间的密集交互。
5. **门控融合和训练损失**：采用**门控融合**来抑制噪声，并使用**三元组损失**来最大化正样本和负样本之间的距离，从而优化模型训练。

# 3 Quota
![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20241223214410.png)

# 4 Q&A
+ Quota表的含义要弄懂
+ 细粒度
+ 视觉枢纽特征？