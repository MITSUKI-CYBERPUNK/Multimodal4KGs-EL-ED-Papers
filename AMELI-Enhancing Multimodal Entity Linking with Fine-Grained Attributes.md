>EACL 2024
# 1 Motivation
+ 多模态任务中，仅依靠文本和图像描述往往不足以准确区分和链接到目标实体，因为知识库中的许多实体可能在文本和视觉上非常相似，只有通过细粒度的属性信息才能进行有效区分
+ 利用实体细粒度属性来提高实体链接的准确性

# 2 Contribution
+ 构建了AMELI数据集：首个支持属性感知MMEL任务的基准数据集
	+ 一个多模式知识库，其中包括从百思买1网站收集的35,598个产品实体，每个实体都用产品名称、产品描述、一组产品属性和值以及几张产品图像
	+ 一个多模态实体链接基准数据集，该数据集包含18,472个数据实例，而每个实例包含特定实体提及的文本描述和几个图像
+ 结合了基于自然语言推理(NLI)的文本模型和基于对比表示学习的图像模型，以在AMELI上建立基线性能


# 3 Methodology
## 3.1 准备工作
### 3.1.1 数据来源&预处理
利用Request5包和Selenium6爬取bestbuy网站上的相关信息：每个产品都用一个产品名称、一个产品类别列表、一段产品描述、一组产品属性和值以及几张图片进行描述4。此外，用户可以在每个产品下以文字和/或图片的形式发布评论，同时每条评论也可以被其他用户评为有用或无用。![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20250112154918.png)

预处理包含：
+ 删除没有图像的评论和产品
+ 移除超过500个token的评论(大多数最先进的预训练语言模型只能处理512个token)
+ 删除“无帮助”标记的评论
+ 验证评论与对应产品的链接，删除无效链接

### 3.1.2 提及检测
提取所有候选产品名称:
+ 获取根词，利用spacy9将根词的名词块识别为候选产品名称
+ 基于**SBERT**计算每个候选提及与目标产品标题之间的相似度，最高者为提及

### 3.1.3 过滤不含信息的评论
提取四个特征：
+ 提到属性的数量(即基于字符串匹配的评论中提到的产品属性的数量)
+ 基于图像的相似性(即基于CLIP (Radford等，2021)图像嵌入的评论图像与产品图像之间的最大相似度)
+ 基于描述的相似性(即基于SBERT的产品描述和评论文本之间的相似性(Reimers和Gurevych, 2019))
+ 基于标题的相似性(即使用SBERT的产品标题和评论文本之间的相似性(Reimers和Gurevych, 2019))

手工标注了500对评论和产品，而每对被分配一个标签:
	如果评论信息足够丰富，可以正确地将提及与目标产品联系起来，则为正面，否则为负面，并使用它们来评估基于阈值的方法，该方法预测评论为无信息评论，如果提取的四个特征得分（提到属性的{#，基于图像的相似性，基于描述的相似性，基于标题的相似性}）不超过四个相应的阈值，即为这些示例上搜索的超参数

### 3.1.4 训练/Dev/测试分裂
Train:Dev:Test = 75%:10%:15%

为了方便人类消除实体歧义，对于每条评论，我们设计了两种策略来自动从知识库中检索强负面实体候选项:
1. 由于我们知道每条评论的目标产品，我们首先从知识库中检索与目标项目最相似的top-K实体作为负面候选项。在这里，两个产品之间的相似性是根据它们的标题使用SBERT计算的(Reimers和Gurevych, 2019);
2. 相似度，我们还使用CLIP基于目标产品的图像检索top-K相似实体(Radford等人，2021)

>这里经常会作表来展示数据集规模，分布等等

## 3.2 Methodology
### 3.2.1 问题表述
![fc009f252d5def956e6fed9c877909c.jpg](https://aquazone.oss-cn-guangzhou.aliyuncs.com/fc009f252d5def956e6fed9c877909c.jpg)

### 3.2.2 先验概率计算
先用spacy14从实体标题和实体类别中提取名词块，并计算名词块到相应实体P(e|m)的先验概率和名词块到实体类别P(c|m)的先验概率

### 3.2.3 属性值提取
由于图像中可能存在文本：
	采用了现成的OCR工具来识别每个评审图像
	中的文本。如果文本与目标KB中实体的任何
	属性值相匹配，我们将其作为提及的属性值

进一步使用GPT-2提取属性值：
+ 输入格式为：“属性值提取:\n #Review_text \n #Attribute_key:”
+ 只保留生成的可以与目标KB中实体的任何属性值匹配的属性

### 3.2.4 候选实体检索
![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20250123171718.png)
首先从目标KB中检索一组候选实体，以便根据文本和视觉相似性将搜索空间浓缩为最相关的实体：
+ 应用**SBERT** (Devlin et al.， 2018)来**接收提及文本和每个实体描述并输出它们的表示**，这些表示被馈送到**余弦相似度函数**中，以选择相似度得分最高的前1000个实体候选。
+ 我们进一步**将提及文本与每个实体描述配对**，并将这些配对馈送到基于bert的(De-vlin et al.， 2018)交叉编码器模型中，其线性分类层输出**交叉编码器相似度得分**。
+ 为了**结合视觉相似性**，我们使用**CLIP** (Radford等，2021)来获取图像表示，然后是使用**余弦相似度的top-J检索**步骤，如前所述。
+ 在**文本余弦相似度分数**、**交叉编码器相似度分数**和**视觉余弦相似度分数**中应用**加权和**以获得合并的相似度分数，这些相似度分数用于选择top-K (K = 10)个实体候选，然后过滤P(e|m)和P(c|m)等于0的候选

## 4.4 实体消歧
![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20250123172221.png)
先前的实体消歧工作仅使用描述性文本和图像表示来消歧。相比之下，我们的方法考虑了**结构化的实体属性文本**

### 4.4.1 基于文本的消歧
基于NLI，自然语言推理，动机是：**如果评论中提到了产品属性值，那么评论文本应该暗示产品属性值**
+ 对于每条评论，给出候选实体及其属性值，我们将每个实体属性/实体描述与审查文本描述配对，并将每对配对提供给**DeBERTa (He et al.，2023)编码器**，以获得它们的上下文表示![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20250123173358.png)
+ 对于每个具有**多个属性值**的实体，我们将从DeBERTa获得的所有上下文表示**连接**起来，并通过**MLP**提供它来预测最终的NLI分数![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20250123173656.png)
	：代表连接
+ 在训练过程中，我们基于**交叉熵目标**优化了基于文本的消歧模块![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20250123173821.png)
	e+为黄金实体，K为检索到的候选实体数量

### 4.4.2 基于图像的消歧
将候选实体输入CLIP获取图像表示，我们通过一个由前馈层和残差连接组成的适配器来提供这些，以使通用图像表示适应面向任务的语义空间![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20250123174225.png)
基于余弦相似度分数应用以下对比损失函数![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20250123174308.png)
	其中B是当前批处理中所有实体的集合，因为我们利用批内否定来提高模型区分黄金实体和否定实体的能力

推理过程：![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20250123174417.png)

# 4 Quota
![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20250123174516.png)


# 5 Q&A


# 6 Revelation
SBERT
CLIP etc MM

